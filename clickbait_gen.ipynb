{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "clickbait_gen.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKkuN_kuaA-a"
      },
      "source": [
        "!pip install sentencepiece transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPe-ju-qdQP7"
      },
      "source": [
        "!pip install rouge_score rouge_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYLxfrj-GeAH"
      },
      "source": [
        "!pip install pytorch_lightning"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyXQ7Cf8ko0B"
      },
      "source": [
        "!pip install wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvvtRUTokaBN"
      },
      "source": [
        "import wandb\n",
        "\n",
        "wandb.login()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClnQt_JFGkc0"
      },
      "source": [
        "!pip install nlp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTik5WzHDhOv"
      },
      "source": [
        "import argparse\n",
        "import glob\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import logging\n",
        "import random\n",
        "import re\n",
        "from itertools import chain\n",
        "from string import punctuation\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from nlp import load_metric\n",
        "\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    T5ForConditionalGeneration,\n",
        "    T5Tokenizer,\n",
        "    get_linear_schedule_with_warmup\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_FtrGZGj5Fg"
      },
      "source": [
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "X, y = np.array(joblib.load(\"sents_hw.pkl\")), np.array(joblib.load(\"sents.pkl\"))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.95)\n",
        "\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbwWQIXlsdID"
      },
      "source": [
        "\n",
        "class CBDS(Dataset):\n",
        "    def __init__(self, X, y, tokenizer, type_path, num_samples, input_length, output_length, print_text=False):         \n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.input_length = input_length\n",
        "        self.tokenizer = tokenizer\n",
        "        self.output_length = output_length\n",
        "        self.print_text = print_text\n",
        "  \n",
        "    def __len__(self):\n",
        "        return self.dataset.shape[0]\n",
        "    \n",
        "    def clean_text(self, text):\n",
        "        text = text.replace('Example of text:', '')\n",
        "        text = text.replace('Example of Summary:', '')\n",
        "        text = text.replace('\\n','')\n",
        "        text = text.replace('``', '')\n",
        "        text = text.replace('\"', '')\n",
        "        \n",
        "        return text\n",
        "    \n",
        "    \n",
        "    def convert_to_features(self, example_batch):\n",
        "        # Tokenize contexts and questions (as pairs of inputs)\n",
        "        \n",
        "        if self.print_text:\n",
        "            print(\"Input Text: \", self.clean_text(example_batch['text']))\n",
        "#         input_ = self.clean_text(example_batch['text']) + \" </s>\"\n",
        "#         target_ = self.clean_text(example_batch['headline']) + \" </s>\"\n",
        "        \n",
        "        input_ = example_batch['X']\n",
        "        target_ = example_batch['y']\n",
        "        \n",
        "        source = self.tokenizer.batch_encode_plus([input_], max_length=self.input_length, \n",
        "                                                     padding='max_length', truncation=True, return_tensors=\"pt\")\n",
        "        \n",
        "        targets = self.tokenizer.batch_encode_plus([target_], max_length=self.output_length, \n",
        "                                                     padding='max_length', truncation=True, return_tensors=\"pt\")\n",
        "    \n",
        "       \n",
        "        return source, targets\n",
        "  \n",
        "    def __getitem__(self, index):\n",
        "        source, targets = self.convert_to_features({'X': self.X[index], 'y': self.y[index]})\n",
        "        \n",
        "        source_ids = source[\"input_ids\"].squeeze()\n",
        "        target_ids = targets[\"input_ids\"].squeeze()\n",
        "\n",
        "        src_mask    = source[\"attention_mask\"].squeeze()\n",
        "        target_mask = targets[\"attention_mask\"].squeeze()\n",
        "\n",
        "        return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids, \"target_mask\": target_mask}\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPz2WVEuIGYg"
      },
      "source": [
        "def get_dataset(X, y, tokenizer, type_path, num_samples, args):\n",
        "      return CBDS(X=X, y=y, tokenizer=tokenizer, type_path=type_path, num_samples=num_samples,  input_length=args.max_input_length, \n",
        "                        output_length=args.max_output_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBZ0zdLlGbRn"
      },
      "source": [
        "class T5FineTuner(pl.LightningModule):\n",
        "    def __init__(self, hparams):\n",
        "        super(T5FineTuner, self).__init__()\n",
        "        self.hparams.update(hparams)\n",
        "        self.model = T5ForConditionalGeneration.from_pretrained(hparams['model_name_or_path'])\n",
        "        self.tokenizer = T5Tokenizer.from_pretrained(hparams['tokenizer_name_or_path'])\n",
        "        self.rouge_metric = load_metric('rouge') \n",
        "        self.epoch_num = 1\n",
        "        if self.hparams[\"freeze_embeds\"]:\n",
        "            print(\"bg6\")\n",
        "            self.freeze_embeds()\n",
        "        if self.hparams[\"freeze_encoder\"]:\n",
        "            print(\"bg7\")\n",
        "            self.freeze_params(self.model.get_encoder())\n",
        "            assert_all_frozen(self.model.get_encoder())\n",
        "            \n",
        "            \n",
        "        n_observations_per_split = {\n",
        "            \"train\": self.hparams[\"n_train\"],\n",
        "            \"validation\": self.hparams[\"n_val\"],\n",
        "            \"test\": self.hparams[\"n_test\"],\n",
        "        }\n",
        "        self.n_obs = {k: v if v >= 0 else None for k, v in n_observations_per_split.items()}\n",
        "        \n",
        "    \n",
        "    def freeze_params(self, model):\n",
        "        for par in model.parameters():\n",
        "            par.requires_grad = False\n",
        "            \n",
        "            \n",
        "    def freeze_embeds(self):\n",
        "        \"\"\"Freeze token embeddings and positional embeddings for bart, just token embeddings for t5.\"\"\"\n",
        "        try:\n",
        "            self.freeze_params(self.model.model.shared)\n",
        "            for d in [self.model.model.encoder, self.model.model.decoder]:\n",
        "                freeze_params(d.embed_positions)\n",
        "                freeze_params(d.embed_tokens)\n",
        "        except:\n",
        "            self.freeze_params(self.model.shared)\n",
        "            for d in [self.model.encoder, self.model.decoder]:\n",
        "                self.freeze_params(d.embed_tokens)\n",
        "    \n",
        "    def lmap(self, f, x):\n",
        "        \"\"\"list(map(f, x))\"\"\"\n",
        "        return list(map(f, x))\n",
        "    \n",
        "\n",
        "    def is_logger(self):\n",
        "        return self.trainer.global_rank <= 0\n",
        "    \n",
        "    \n",
        "    def parse_score(self, result):\n",
        "        return {k: round(v.mid.fmeasure * 100, 4) for k, v in result.items()}\n",
        "        \n",
        "    def forward(\n",
        "      self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, lm_labels=None\n",
        "  ):\n",
        "        return self.model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            decoder_input_ids=decoder_input_ids,\n",
        "            decoder_attention_mask=decoder_attention_mask,\n",
        "            labels=lm_labels,\n",
        "    )\n",
        "\n",
        "    def _step(self, batch):\n",
        "        lm_labels = batch[\"target_ids\"]\n",
        "        lm_labels[lm_labels[:, :] == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        outputs = self(\n",
        "            input_ids=batch[\"source_ids\"],\n",
        "            attention_mask=batch[\"source_mask\"],\n",
        "            lm_labels=lm_labels,\n",
        "            decoder_attention_mask=batch['target_mask']\n",
        "        )\n",
        "\n",
        "        loss = outputs[0]\n",
        "\n",
        "        return loss\n",
        "    \n",
        "    \n",
        "    def ids_to_clean_text(self, generated_ids):\n",
        "        gen_text = self.tokenizer.batch_decode(\n",
        "            generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
        "        )\n",
        "        return self.lmap(str.strip, gen_text)\n",
        "    \n",
        "    \n",
        "    def _generative_step(self, batch) :\n",
        "        \n",
        "        t0 = time.time()\n",
        "        \n",
        "        generated_ids = self.model.generate(\n",
        "            batch[\"source_ids\"],\n",
        "            attention_mask=batch[\"source_mask\"],\n",
        "            use_cache=True,\n",
        "            decoder_attention_mask=batch['target_mask'],\n",
        "            max_length=150, \n",
        "            num_beams=2,\n",
        "            repetition_penalty=2.5, \n",
        "            length_penalty=1.0, \n",
        "            early_stopping=True\n",
        "        )\n",
        "        preds = self.ids_to_clean_text(generated_ids)\n",
        "        target = self.ids_to_clean_text(batch[\"target_ids\"])\n",
        "            \n",
        "        gen_time = (time.time() - t0) / batch[\"source_ids\"].shape[0]  \n",
        "    \n",
        "        loss = self._step(batch)\n",
        "        base_metrics = {'val_loss': loss}\n",
        "#         rouge: Dict = self.calc_generative_metrics(preds, target)\n",
        "        summ_len = np.mean(self.lmap(len, generated_ids))\n",
        "        base_metrics.update(gen_time=gen_time, gen_len=summ_len, preds=preds, target=target)\n",
        "        self.rouge_metric.add_batch(preds, target)\n",
        "        \n",
        "#         rouge_results = self.rouge_metric.compute() \n",
        "#         rouge_dict = self.parse_score(rouge_results)\n",
        "#         base_metrics.update(rouge1=rouge_dict['rouge1'], rougeL=rouge_dict['rougeL'])\n",
        "        \n",
        "        return base_metrics\n",
        "    \n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss = self._step(batch)\n",
        "\n",
        "        tensorboard_logs = {\"train_loss\": loss}\n",
        "        return {\"loss\": loss, \"log\": tensorboard_logs}\n",
        "\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        return self._generative_step(batch)\n",
        "    \n",
        "    def training_epoch_end(self, outputs):\n",
        "      self.model.save_pretrained(f\"/content/drive/MyDrive/t5_cb/epoch_{self.epoch_num}\")\n",
        "      self.epoch_num += 1\n",
        "      \n",
        "    def validation_epoch_end(self, outputs):\n",
        "        \n",
        "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
        "        tensorboard_logs = {\"val_loss\": avg_loss}\n",
        "        \n",
        "        rouge_results = self.rouge_metric.compute() \n",
        "        rouge_dict = self.parse_score(rouge_results)\n",
        "    \n",
        "        tensorboard_logs.update(rouge1=rouge_dict['rouge1'], rougeL=rouge_dict['rougeL'])\n",
        "        \n",
        "        ## Clear out the lists for next epoch\n",
        "        self.target_gen= []\n",
        "        self.prediction_gen=[]\n",
        "        return {\"avg_val_loss\": avg_loss, \n",
        "                \"rouge1\" : rouge_results['rouge1'],\n",
        "                \"rougeL\" : rouge_results['rougeL'],\n",
        "                \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"Prepare optimizer and schedule (linear warmup and decay)\"\n",
        "\n",
        "        model = self.model\n",
        "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "        optimizer_grouped_parameters = [\n",
        "            {\n",
        "                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "                \"weight_decay\": self.hparams[\"weight_decay\"],\n",
        "            },\n",
        "            {\n",
        "                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "                \"weight_decay\": 0.0,\n",
        "            },\n",
        "        ]\n",
        "        optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams[\"learning_rate\"], eps=self.hparams[\"adam_epsilon\"])\n",
        "        self.opt = optimizer\n",
        "        return [optimizer]\n",
        "  \n",
        "  \n",
        "    def get_tqdm_dict(self):\n",
        "        tqdm_dict = {\"loss\": \"{:.3f}\".format(self.trainer.avg_loss), \"lr\": self.lr_scheduler.get_last_lr()[-1]}\n",
        "\n",
        "        return tqdm_dict\n",
        "    \n",
        "\n",
        "    def train_dataloader(self):   \n",
        "        n_samples = self.n_obs['train']\n",
        "        train_dataset = get_dataset(X_train, y_train, tokenizer=self.tokenizer, type_path=\"train\", num_samples=n_samples, args=self.hparams)\n",
        "        dataloader = DataLoader(train_dataset, batch_size=self.hparams[\"train_batch_size\"], drop_last=True, shuffle=True, num_workers=4)\n",
        "        t_total = (\n",
        "            (len(X) // (self.hparams[\"train_batch_size\"] * max(1, self.hparams[\"n_gpu\"])))\n",
        "            // self.hparams[\"gradient_accumulation_steps\"]\n",
        "            * float(self.hparams[\"num_train_epochs\"])\n",
        "        )\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            self.opt, num_warmup_steps=self.hparams[\"warmup_steps\"], num_training_steps=t_total\n",
        "        )\n",
        "        self.lr_scheduler = scheduler\n",
        "        return dataloader\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        n_samples = self.n_obs['validation']\n",
        "        validation_dataset = get_dataset(X_test[:400], y_test[:400], tokenizer=self.tokenizer, type_path=\"validation\", num_samples=n_samples, args=self.hparams)\n",
        "        \n",
        "        return DataLoader(validation_dataset, batch_size=self.hparams[\"eval_batch_size\"], num_workers=4)\n",
        "    \n",
        "    \n",
        "    def test_dataloader(self):\n",
        "        n_samples = self.n_obs['test']\n",
        "        test_dataset = get_dataset(X_test[400:], y_test[400:], tokenizer=self.tokenizer, type_path=\"test\", num_samples=n_samples, args=self.hparams)\n",
        "        \n",
        "        return DataLoader(test_dataset, batch_size=self.hparams[\"eval_batch_size\"], num_workers=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQ_yIWqzIwtn"
      },
      "source": [
        "args_dict = dict(\n",
        "    output_dir=\"\", # path to save the checkpoints\n",
        "    model_name_or_path='t5-small',\n",
        "    tokenizer_name_or_path='t5-small',\n",
        "    max_input_length=512,\n",
        "    max_output_length=150,\n",
        "    freeze_encoder=False,\n",
        "    freeze_embeds=False,\n",
        "    learning_rate=3e-4,\n",
        "    weight_decay=0.0,\n",
        "    adam_epsilon=1e-8,\n",
        "    warmup_steps=0,\n",
        "    train_batch_size=4,\n",
        "    eval_batch_size=4,\n",
        "    num_train_epochs=2,\n",
        "    gradient_accumulation_steps=8,\n",
        "    n_gpu=1,\n",
        "    resume_from_checkpoint=None, \n",
        "    val_check_interval = 0.05, \n",
        "    n_val=200,\n",
        "    n_train=-1,\n",
        "    n_test=-1,\n",
        "    early_stop_callback=False,\n",
        "    fp_16=False, # if you want to enable 16-bit training then install apex and set this to true\n",
        "    opt_level='O1', # you can find out more on optimisation levels here https://nvidia.github.io/apex/amp.html#opt-levels-and-properties\n",
        "    max_grad_norm=1.0, # if you enable 16-bit training then set this to a sensible value, 0.5 is a good default\n",
        "    seed=42,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6JZ3FqFI5Rw"
      },
      "source": [
        "args_dict.update({'output_dir': '/content/drive/MyDrive/t5_cb', 'num_train_epochs':2,\n",
        "                 'train_batch_size': 4, 'eval_batch_size': 4})\n",
        "args = argparse.Namespace(**args_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kOHF5WfPxHS"
      },
      "source": [
        "model = T5FineTuner(args_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RI8nAX3kQdW"
      },
      "source": [
        "wandb_logger = WandbLogger(project='cb-t5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwsN5R5enHY3"
      },
      "source": [
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class LoggingCallback(pl.Callback):\n",
        "    def on_validation_end(self, trainer, pl_module):\n",
        "        logger.info(\"***** Validation results *****\")\n",
        "        if pl_module.is_logger():\n",
        "            metrics = trainer.callback_metrics\n",
        "            # Log results\n",
        "            for key in sorted(metrics):\n",
        "                if key not in [\"log\", \"progress_bar\"]:\n",
        "                    logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
        "\n",
        "    def on_test_end(self, trainer, pl_module):\n",
        "        logger.info(\"***** Test results *****\")\n",
        "\n",
        "        if pl_module.is_logger():\n",
        "            metrics = trainer.callback_metrics\n",
        "\n",
        "            # Log and save results to file\n",
        "            output_test_results_file = os.path.join(pl_module.hparams.output_dir, \"test_results.txt\")\n",
        "            with open(output_test_results_file, \"w\") as writer:\n",
        "                for key in sorted(metrics):\n",
        "                    if key not in [\"log\", \"progress_bar\"]:\n",
        "                        logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
        "                        writer.write(\"{} = {}\\n\".format(key, str(metrics[key])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQkX9kbgQuBC"
      },
      "source": [
        "## Define Checkpoint function\n",
        "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
        "    filename=args.output_dir, monitor=\"val_loss\", mode=\"min\", save_top_k=3\n",
        ")\n",
        "\n",
        "## If resuming from checkpoint, add an arg resume_from_checkpoint\n",
        "train_params = dict(\n",
        "    accumulate_grad_batches=args.gradient_accumulation_steps,\n",
        "    gpus=args.n_gpu,\n",
        "    max_epochs=args.num_train_epochs,\n",
        "    precision= 16 if args.fp_16 else 32,\n",
        "    amp_level=args.opt_level,\n",
        "    resume_from_checkpoint=args.resume_from_checkpoint,\n",
        "    gradient_clip_val=args.max_grad_norm,\n",
        "    checkpoint_callback=True,\n",
        "    val_check_interval=args.val_check_interval,\n",
        "    logger=wandb_logger,\n",
        "    callbacks=[LoggingCallback()],\n",
        ")\n",
        "\n",
        "trainer = pl.Trainer(**train_params)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alsV4SlcjmcW"
      },
      "source": [
        "trainer.fit(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7G2bwD6nSD1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}